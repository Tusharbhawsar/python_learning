{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f7f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I love machine learning\",\n",
    "    \"I love deep learning\",\n",
    "    \"machine learning is fun\",\n",
    "    \"deep learning is a branch of machine learning\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98549712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^a-z\\s]', '', sentence)\n",
    "    return sentence.split()\n",
    "\n",
    "tokenized_corpus = [tokenize(sentence) for sentence in corpus]\n",
    "print(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e589df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "vocab = set(word for sentence in tokenized_corpus for word in sentence)\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(vocab)\n",
    "print(word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f1ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram_pairs(tokenized_corpus, window_size=2):\n",
    "    pairs = []\n",
    "    for sentence in tokenized_corpus:\n",
    "        for idx, target_word in enumerate(sentence):\n",
    "            context_range = list(range(max(0, idx - window_size), min(len(sentence), idx + window_size + 1)))\n",
    "            for context_idx in context_range:\n",
    "                if context_idx != idx:\n",
    "                    pairs.append((target_word, sentence[context_idx]))\n",
    "    return pairs\n",
    "\n",
    "pairs = generate_skipgram_pairs(tokenized_corpus)\n",
    "print(pairs[:10])  # sample output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646ebf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(word, word2idx):\n",
    "    vec = np.zeros(len(word2idx))\n",
    "    vec[word2idx[word]] = 1\n",
    "    return vec\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for target, context in pairs:\n",
    "    X.append(one_hot_encode(target, word2idx))\n",
    "    Y.append(one_hot_encode(context, word2idx))\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "# print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f8fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list\n",
    "# # !python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embedding = nn.Linear(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "embedding_dim = 10\n",
    "model = Word2Vec(vocab_size, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Model Training (REVISED using Softmax and proper targets)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "X_tensor = torch.FloatTensor(X)  # input: one-hot encoded\n",
    "Y_tensor = torch.LongTensor([np.argmax(y) for y in Y])  # labels: class index\n",
    "\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_tensor)  # raw scores (logits), shape: [batch, vocab_size]\n",
    "    loss = loss_fn(outputs, Y_tensor)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14afe8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embedding.weight.data.numpy()\n",
    "for word, idx in word2idx.items():\n",
    "    print(f\"{word}: {embeddings[idx]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ecb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4613e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = \"machine\"\n",
    "word2 = \"learning\"\n",
    "\n",
    "vec1 = embeddings[word2idx[word1]]\n",
    "vec2 = embeddings[word2idx[word2]]\n",
    "\n",
    "similarity = cosine_similarity(vec1, vec2)\n",
    "print(f\"Cosine similarity between '{word1}' and '{word2}': {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeef86f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 1],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [[1, 0], [0, 1]]\n",
    "b = [[4, 1], [2, 2]]\n",
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92680397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
